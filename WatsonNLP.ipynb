{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn\n",
    "#Yall know what these libraries are all for, seaborn if we want to get fancy with plotting\n",
    "\n",
    "from sklearn import model_selection\n",
    "#Cross validation, test-train split, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRaw = pd.read_csv(\"train.csv\")\n",
    "testRaw = pd.read_csv(\"test.csv\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_phrase(s):\n",
    "   tokens = list(tokenizer.tokenize(s))\n",
    "   tokens.append('[SEP]')\n",
    "   return tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n",
    "    print('Number of replicas:', strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(phrase_ones, phrase_twos, tokenizer):\n",
    "    \n",
    "  num_examples = len(phrase_ones)\n",
    "  \n",
    "  sentence1 = tf.ragged.constant([\n",
    "      encode_phrase(s)\n",
    "      for s in np.array(phrase_ones)])\n",
    "  sentence2 = tf.ragged.constant([\n",
    "      encode_phrase(s)\n",
    "       for s in np.array(phrase_twos)])\n",
    "\n",
    "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
    "  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
    "\n",
    "  input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "\n",
    "  type_cls = tf.zeros_like(cls)\n",
    "  type_s1 = tf.zeros_like(sentence1)\n",
    "  type_s2 = tf.ones_like(sentence2)\n",
    "  input_type_ids = tf.concat(\n",
    "      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
    "\n",
    "  inputs = {\n",
    "      'input_word_ids': input_word_ids.to_tensor(),\n",
    "      'input_mask': input_mask,\n",
    "      'input_type_ids': input_type_ids}\n",
    "\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(trainRaw.premise.values, trainRaw.hypothesis.values, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2993cc858b770212d255d4a67ebe491a9587de5d4a2b2894d3038b229a1d24a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
